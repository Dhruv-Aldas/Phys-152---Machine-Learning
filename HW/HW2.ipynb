{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1648752376661,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "WKo_jWSLI6PC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import torch.utils.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z6AbT6VTooP"
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "Use `np.histogram` to calculate the [probability density](https://en.wikipedia.org/wiki/Probability_density_function) that values in an arbitrary input data array fall within user-specified bins. Hint: `np.histogram` does all the work for you with the correct arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1648752809896,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "txgSi6EUTqXX",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def estimate_probability_density(data, bins):\n",
    "    \"\"\"Estimate the probability density of arbitrary data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array\n",
    "        1D numpy array of random values.\n",
    "    bins : array\n",
    "        1D numpy array of N+1 bin edges to use. Must be increasing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        1D numpy array of N probability densities.\n",
    "    \"\"\"\n",
    "    assert np.all(np.diff(bins) > 0)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    counts, bins = np.histogram(data, bins, density=True)\n",
    "    \n",
    "    return counts\n",
    "    \n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1648752810060,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "2xOvjCPZT0hC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "generator = np.random.RandomState(seed=123)\n",
    "data = generator.uniform(size=100)\n",
    "bins = np.linspace(0., 1., 11)\n",
    "rho = estimate_probability_density(data, bins)\n",
    "assert np.allclose(rho, [ 0.6,  0.8,  0.7,  1.7,  1.1,  1.3,  1.6,  0.9,  0.8,  0.5])\n",
    "data = generator.uniform(size=1000)\n",
    "bins = np.linspace(0., 1., 101)\n",
    "rho = estimate_probability_density(data, bins)\n",
    "dx = bins[1] - bins[0]\n",
    "assert np.allclose(dx * rho.sum(), 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9iMtapqRZcM"
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "Define a function to calculate the [entropy](https://en.wikipedia.org/wiki/Entropy_estimation) $H(\\rho)$ of a binned probability density, defined as:\n",
    "\n",
    "$$H(\\rho) \\equiv -\\sum_i \\rho_i \\log(\\rho_i) \\Delta w_i$$\n",
    "\n",
    "where $\\rho_i$ is the binned density in bin $i$ with width $w_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1648752813865,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "E1oPDWPcRzUY"
   },
   "outputs": [],
   "source": [
    "def binned_entropy(rho, bins):\n",
    "    \"\"\"Calculate the binned entropy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rho : array\n",
    "        1D numpy array of densities, e.g., calculated by the previous function.\n",
    "    bins : array\n",
    "        1D numpy array of N+1 bin edges to use. Must be increasing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Value of the binned entropy.\n",
    "    \"\"\"\n",
    "    assert np.all(np.diff(bins) > 0)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    widths = np.empty(0) # creating list of widths of each bin\n",
    "    for i in range(len(bins) - 1):\n",
    "        widths = np.append(widths, bins[i+1] - bins[i])\n",
    "    \n",
    "    return -np.sum(rho * np.log(rho) * widths)\n",
    "    \n",
    "    raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 178,
     "status": "ok",
     "timestamp": 1648752814222,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "eHDTdUe_R5j_"
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "generator = np.random.RandomState(seed=123)\n",
    "data1 = generator.uniform(size=10000)\n",
    "data2 = generator.uniform(size=10000) ** 4\n",
    "bins = np.linspace(0., 1., 11)\n",
    "rho1 = estimate_probability_density(data1, bins)\n",
    "rho2 = estimate_probability_density(data2, bins)\n",
    "H1 = binned_entropy(rho1, bins)\n",
    "H2 = binned_entropy(rho2, bins)\n",
    "assert np.allclose(H1, -0.000801544)\n",
    "assert np.allclose(H2, -0.699349908)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UH9jbZTR2wN"
   },
   "source": [
    "# Problem 3\n",
    "\n",
    "We're going to implement a network that will do multi-category classification.  We'll base this on the tools we developed in the [NeuralNetworks2](https://git.ucsc.edu/mhance/phys152/-/blob/master/Notebooks/NeuralNetworks2.ipynb) notebook from class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 6686,
     "status": "ok",
     "timestamp": 1648852688955,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "kx_jGSlTEwSF"
   },
   "source": [
    "First let's load in the data file.  You can find the data file in the HW area of my gitlab repo: [HW2_data.h5](https://git.ucsc.edu/mhance/phys152/-/blob/master/HW/HW2_data.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1648852689102,
     "user": {
      "displayName": "Michael Hance",
      "userId": "01498427894019643105"
     },
     "user_tz": 420
    },
    "id": "grgFGAURGLGP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "(1000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1774199e130>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "h5f = h5py.File('HW2_data.h5','r')\n",
    "x_train= h5f['x_train'][:]\n",
    "y_train= h5f['y_train'][:]\n",
    "x_test= h5f['x_test'][:]\n",
    "y_test= h5f['y_test'][:]\n",
    "h5f.close()\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 4)\n",
      "(1000,)\n",
      "[0 0 1 1 2 2 2 2 2 1 1 1 1 1 2 1 0 1 0 2 2 1 0 2 2 1 2 1 0 2 2 2 2 1 1 1 0\n",
      " 2 2 1 0 2 0 0 0 2 1 1 0 1 1 2 0 2 1 2 0 0 0 2 1 1 2 0 1 2 1 0 2 2 1 0 0 0\n",
      " 1 2 0 2 0 1 1 2 1 2 0 1 0 0 2 0 0 0 1 0 0 2 1 0 1 0 0 2 2 0 0 0 1 2 2 2 0\n",
      " 2 0 1 1 2 2 1 0 2 0 0 0 2 0 0 0 1 0 0 1 0 0 2 2 2 2 0 0 2 2 1 2 0 2 2 2 0\n",
      " 1 1 1 1 2 2 0 1 1 0 1 1 2 2 2 0 1 2 0 1 0 0 2 0 0 1 0 0 0 0 2 1 0 2 1 0 2\n",
      " 0 0 0 1 0 1 0 0 2 0 1 2 0 2 2 0 0 0 1 0 1 0 1 0 2 0 0 0 1 1 1 0 2 0 1 1 2\n",
      " 0 1 2 0 1 2 0 2 2 1 2 1 1 2 0 1 1 0 1 1 1 2 1 0 1 0 1 0 1 2 0 1 1 2 1 1 0\n",
      " 0 2 2 0 0 1 2 0 1 0 1 2 0 0 1 2 0 1 0 1 0 2 2 2 1 0 1 0 0 0 1 0 0 0 1 0 0\n",
      " 2 2 2 2 2 1 2 2 1 2 1 2 1 2 1 2 2 0 0 2 2 1 2 2 0 0 1 2 2 2 2 0 1 1 2 2 2\n",
      " 1 2 2 0 1 1 1 1 0 1 2 2 1 1 0 2 0 0 0 1 1 2 2 1 2 0 2 2 2 1 0 2 1 2 2 2 0\n",
      " 1 0 2 2 1 1 0 0 1 1 1 1 0 1 1 2 0 1 0 1 2 2 0 0 1 0 0 2 1 0 1 2 2 1 0 1 2\n",
      " 0 0 2 2 2 1 2 0 0 1 0 2 1 1 2 2 2 1 0 0 2 0 0 1 0 1 1 0 1 1 1 1 1 2 2 0 2\n",
      " 0 0 0 2 2 1 1 1 0 0 1 1 2 2 1 1 0 0 0 1 1 2 0 0 2 0 1 1 1 0 0 1 0 1 0 0 0\n",
      " 2 1 0 0 2 2 0 2 1 0 2 2 1 1 2 1 0 1 2 0 2 0 1 0 0 1 0 0 1 0 0 0 0 0 1 2 1\n",
      " 1 0 0 2 0 1 2 0 0 1 0 1 1 1 1 2 0 1 1 0 2 1 2 2 2 2 0 2 0 0 0 2 0 0 0 1 1\n",
      " 1 2 1 1 1 1 2 2 1 2 1 2 1 0 1 1 0 1 2 0 2 1 0 1 2 0 0 0 0 0 2 1 1 0 1 2 0\n",
      " 0 0 1 1 1 1 1 2 2 0 2 1 2 1 1 0 1 2 0 2 0 0 2 2 2 1 0 2 1 0 0 0 1 2 2 1 2\n",
      " 1 2 1 2 2 0 2 0 2 1 1 1 1 2 0 0 0 2 1 1 0 0 1 1 1 2 1 0 0 2 2 2 1 2 0 1 2\n",
      " 1 0 1 2 2 1 0 0 2 1 0 1 2 1 0 1 1 2 2 2 2 1 0 0 0 2 0 0 0 2 0 2 2 1 0 2 1\n",
      " 0 0 1 1 0 1 2 1 1 0 0 2 2 0 2 1 0 0 2 0 2 0 1 1 1 0 2 0 0 0 2 2 1 2 0 0 2\n",
      " 2 2 1 0 2 0 0 2 1 0 1 2 2 0 1 1 1 0 2 0 1 2 0 0 0 1 2 0 1 0 2 0 1 0 2 2 1\n",
      " 0 2 2 1 0 0 1 1 2 0 0 0 2 1 0 1 1 1 2 0 0 1 1 1 0 2 0 2 0 1 2 1 0 0 2 0 2\n",
      " 0 2 2 0 2 1 2 2 2 2 2 2 0 0 1 0 0 0 0 0 0 0 0 2 2 1 1 1 1 2 2 1 1 0 1 2 1\n",
      " 2 1 0 1 1 1 2 1 1 2 2 0 1 0 0 0 2 2 2 1 2 2 0 0 0 1 0 0 1 0 0 0 1 0 2 1 2\n",
      " 2 1 2 1 0 1 1 0 0 2 1 2 0 0 2 2 2 2 1 2 2 1 1 2 2 2 2 1 2 2 0 2 1 0 0 0 1\n",
      " 2 1 0 1 2 0 0 0 0 1 1 1 0 2 2 2 1 1 2 0 0 1 2 0 0 2 1 1 0 0 2 0 2 2 1 1 0\n",
      " 1 2 1 1 2 1 2 1 1 2 2 0 0 1 1 1 0 0 1 2 2 0 0 0 0 2 1 0 1 0 0 2 2 2 0 1 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(x_train))\n",
    "print(np.shape(y_train))\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([354.,   0., 328.,   0., 318.,   0.]),\n",
       " array([0. , 0.5, 1. , 1.5, 2. , 2.5, 3. ]),\n",
       " <BarContainer object of 6 artists>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(y_train,bins=(np.max(y_train)+1)*2,range=(0,np.max(y_train)+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Rp7v2yLGVDv"
   },
   "source": [
    "Now the fun part.  Let's construct a similar network to what we used in class: \n",
    "- 2 hidden layers, with 20 and 25 nodes respectively, using ReLU activation functions.  \n",
    "- The number of input layers should correspond to the number of features in the training/test data, and the number of output layers should correspond to the number of categories in the training/test data.  In this case, the `y_train` and `y_test` lists will show the category label as a number between `0` and `C`, where `C` is the total number of categories.  You should inspect the training/test data to figure out the number of features and the number of categories.\n",
    "- For the loss function, we should use `torch.nn.CrossEntropyLoss()`.  This implements the `softmax` activation function on the output layer that we'll use for multi-category classification.\n",
    "- Let's use the Adam optimizer, starting with a learning rate of 0.1\n",
    "- Break the training data into 5 batches and run for 1000 epochs\n",
    "\n",
    "Then you can train the network!  Keep track of the loss values for the training and test data, and plot them vs the epoch number after the training is completed.\n",
    "\n",
    "The predictions of the network will be a set of `C` numbers for each event, corresponding to the probability that the event is classified as each of the `C` different categories.  We should take the maximum value for each set of `C` numbers to determine the category label.  So if the output for event 0 looks like `[0.8, 0.6, 0.95, 0.10]` and the categories are numbered `0..3` then the predicted label would be `2`.\n",
    "\n",
    "Once the network is trained, let's inspect the output.  Using the predictions for the training data, construct a \"confusion matrix\", showing the true labels on one axis and the predicted labels on the other.\n",
    "\n",
    "I can offer more clarifications on this as the HW period goes on, so feel free to ask questions as you start on this one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# creating the net\n",
    "\n",
    "net = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 20), # 4 input features going into 20 nodes\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(20, 25), # 20 nodes into 25 nodes\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(25, 1), # 1 output\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'net.pth') # saving the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizier = torch.optim.Adam( net.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = np.empty(0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP3ns07cB9JHhUEFAuxihak",
   "name": "HW2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9df52092",
   "metadata": {},
   "source": [
    "# Homework 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "70aae716",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import os.path\n",
    "from os.path import exists\n",
    "import os\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import urllib.request\n",
    "\n",
    "device=\"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    device=\"cuda:0\"\n",
    "print(device)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80193b92",
   "metadata": {},
   "source": [
    "## Problem 1 (40 points): Revisiting our CNN from lecture\n",
    "\n",
    "Let's revisit the CNN notebook that we worked through in class, and modify the code there to do something slightly different.\n",
    "\n",
    "First, let's import our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "0e179ee2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x14e4aee4b70>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = 'ising_data.npz'\n",
    "\n",
    "if not exists(output):\n",
    "    import gdown\n",
    "    url = \"https://drive.google.com/u/0/uc?export=download&confirm=HgGH&id=1Ihxt1hb3Kyv0IrjHlsYb9x9QY7l7n2Sl\"\n",
    "    gdown.download(url, output, quiet=True)\n",
    "\n",
    "f = np.load(output, allow_pickle=True)\n",
    "n_train = 20000\n",
    "n_test=len(f[\"C\"])-n_train\n",
    "\n",
    "x_train, x_test = f[\"C\"][:n_train], f[\"C\"][n_train:]\n",
    "y_train, y_test = f[\"T\"][:n_train], f[\"T\"][n_train:]\n",
    "\n",
    "x_train_tensor=torch.tensor(x_train,dtype=torch.float).unsqueeze(1)\n",
    "y_train_tensor=torch.tensor(y_train,dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "x_test_tensor=torch.tensor(x_test,dtype=torch.float).unsqueeze(1)\n",
    "y_test_tensor=torch.tensor(y_test,dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "xy_train = torch.utils.data.TensorDataset(x_train_tensor.float(),y_train_tensor)\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64479c70",
   "metadata": {},
   "source": [
    "In the CNN lecture notebook, we used a CNN to predict whether the image had crossed the phase transition boundary or not.  What I'd like to do is a followup that I may have mentioned in lecture: let's use the CNN to predict the temperature that corresponds to the image, instead of a binary \"yes/no\" decision.  Some things to consider:\n",
    "\n",
    "* What should the loss function be?\n",
    "\n",
    "**Now your job**: setup and train a CNN that predicts the temperature associated with each image.  Here are some requirements, which may be different from what was done in the CNN notebook in class:\n",
    "\n",
    "* Have the first convolutional layer use 16 filters\n",
    "* Have the second convolutional layer use 32 filters\n",
    "* Have the fully-connected part at the end include two layers with 32 nodes\n",
    "* Use the `Adam` optimizer with a learning rate of 0.01\n",
    "* Run for 10 epochs\n",
    "\n",
    "\n",
    "As part of this problem, you should produce:\n",
    "* The normal loss plot showing the training and test losses vs the number of epochs\n",
    "* Once the network is trained, pass the test data through the network again, and get the predicted values.  Compare the predicted values with the true values; plot the difference and find the bias and variance of the network.\n",
    "\n",
    "Some hints:\n",
    "* You don't necessarily need to run the code for the full 16+32 convolutional filter network every time, the training takes a little while!  Feel free to debug with smaller numbers of filters.\n",
    "* However, the setup is a bit delicate, so even small changes in the hyperparameters specified above may cause the training to not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "07915cb8-af8c-4bd4-9e9c-de734c613ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# code for creating the neural net\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # two convolutional layers\n",
    "        self.conv1 = torch.nn.Conv2d( 1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = torch.nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "\n",
    "        # fully connected layer\n",
    "        self.fc1 = torch.nn.Linear(32, 32)\n",
    "        self.fc2 = torch.nn.Linear(32, 1)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # if we wanted to add another layer before pooling, we could do that here.\n",
    "\n",
    "        # Run max pooling over x\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        # we could also do some dropout here if we wanted\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # perhaps we want another layer here?\n",
    "\n",
    "        # the (1,1) makes this into global average pooling\n",
    "        x = F.adaptive_avg_pool2d(x,(1,1))\n",
    "    \n",
    "        # Flatten x with start_dim=1, i.e. only keep one number per channel per event.\n",
    "        # This one line kept me up into the wee hours the other night!\n",
    "        x=torch.flatten(x,1)\n",
    "      \n",
    "        # Pass data through the fully connected layer.\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d9c65d1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (fc1): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=1, bias=True)\n",
      ")\n",
      "5889\n"
     ]
    }
   ],
   "source": [
    "# create the net, define the loss fuction and optimizer\n",
    "net = Net()\n",
    "print(net)\n",
    "print(np.sum([np.prod(theta.shape) for theta in net.parameters()]))\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65837b8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train the net\n",
    "losses = []\n",
    "losses_test = []\n",
    "\n",
    "loader = torch.utils.data.DataLoader(xy_train, batch_size=int(len(x_train)/5), shuffle=True)\n",
    "\n",
    "for epoch in range(10):\n",
    "    start_time = time.time()\n",
    "    for x_batch, y_batch in loader:\n",
    "        y_pred = net(x_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(loss.data)\n",
    "    net.eval() # configure the model for evaluation (testing)\n",
    "    y_pred = net(x_test_tensor)\n",
    "    test_loss=loss_fn(y_pred, y_test_tensor)\n",
    "    losses_test.append(test_loss.data)\n",
    "    end_time=time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Completed epoch %2d in %4.1fs, Train loss=%4.1e, Test loss=%4.1e\" % (epoch, elapsed_time, loss.data, test_loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1377619c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses'\n",
    "plt.plot(losses, '.', label=\"Train\")\n",
    "plt.plot(losses_test, '.', label=\"Test\")\n",
    "plt.legend()\n",
    "plt.xlabel('Training Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aacb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate and plot the losses for individual training dataset events\n",
    "y_pred = np.array(net(x_test_tensor).data.flatten())\n",
    "\n",
    "T_train, T_test = f[\"T\"][:n_train], f[\"T\"][n_train:]\n",
    "diff = T_test - y_pred\n",
    "\n",
    "plt.scatter(T_test, y_pred)\n",
    "plt.xlabel('True Temperatures')\n",
    "plt.ylabel('PRedicted Temperatures')\n",
    "plt.show()\n",
    "\n",
    "plt.hist(diff,bins=100,range=[-3,3])\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean = %f\" % np.mean(diff))\n",
    "print(\"Variance = %f\" % np.var(diff))\n",
    "print(\"Std Dev = %f\" % np.var(diff)**0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2b77c8",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points): Principal Components Analysis\n",
    "\n",
    "Let's perform a PCA test on some input data that we'll use for Problem #3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51324de9",
   "metadata": {},
   "source": [
    "First let's get some training and test datasets that we can use...  we'll start with one of the files that we used from HW3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6d2c2de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sample(sample):\n",
    "    events=[]\n",
    "    for i in sample:\n",
    "        if i[0]>2:\n",
    "            events.append(i)\n",
    "    return events\n",
    "\n",
    "def get_sample_from_URL(samplename,branches):\n",
    "    \n",
    "    if not os.path.isfile(samplename):\n",
    "        urllib.request.urlretrieve(\"http://mhance.scipp.ucsc.edu/%s\" % samplename, samplename)\n",
    "\n",
    "    alldata=None\n",
    "    with h5py.File(samplename,'r') as hdf5file:\n",
    "        data=hdf5file[list(hdf5file.keys())[0]][\"lowleveltree\"]\n",
    "        num_backgr_events=len(data[\"numjet\"])\n",
    "        alldata = data[branches]    \n",
    "\n",
    "    Alldata=[[float(i) for i in j] for j in alldata]\n",
    "    Alldata_clean=clean_sample(Alldata)\n",
    "    return Alldata_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2c575f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now scale based on the training data:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "branches=(\"numjet\",\"numlepton\",\"numbtagjet\",\n",
    "          \"met\",\"metphi\",\n",
    "          \"jet1pT\", \"jet1eta\", \"jet1phi\",\"jet1b\",\n",
    "          \"jet2pT\", \"jet2eta\", \"jet2phi\",\"jet2b\",\n",
    "          \"jet3pT\", \"jet3eta\", \"jet3phi\",\"jet3b\")\n",
    "\n",
    "traindata=get_sample_from_URL('lowlevelAna_ttbar.hf5',branches)\n",
    "x_train = sc.fit_transform(traindata)\n",
    "train=torch.tensor(x_train,dtype=torch.float)\n",
    "\n",
    "testdata=get_sample_from_URL('lowlevelAna_test.hf5',branches)\n",
    "x_test = sc.transform(testdata)\n",
    "test=torch.tensor(x_test,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a1e49",
   "metadata": {},
   "source": [
    "Using the `scikit-learn` PCA tool, analyze the `ttbar` dataset (`x_train`) retrieved above.\n",
    "\n",
    "* Find the number of components needed to explain 50% of the variance.\n",
    "* List the magnitudes of that number of leading eigenvalues, as a fraction of the total variance they capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "948d75d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15115807 0.2541584  0.3479097  0.43165682 0.51334968 0.57964796\n",
      " 0.64179913 0.70299069 0.76102156 0.8083036  0.85089644 0.88876157\n",
      " 0.92450541 0.95757566 0.97989425 0.99199516 1.        ]\n",
      "[2.56995718 1.75118951 1.59393956 1.42385066 1.3889244  1.12718924\n",
      " 1.0566808  1.04036593 0.98662837 0.80387912 0.72415433 0.64377484\n",
      " 0.60770917 0.56225322 0.37945587 0.20573718 0.13609652]\n",
      "[0.15115807 0.10300033 0.0937513  0.08374712 0.08169285 0.06629828\n",
      " 0.06215116 0.06119157 0.05803087 0.04728204 0.04259284 0.03786513\n",
      " 0.03574384 0.03307024 0.02231859 0.01210092 0.00800484]\n",
      "eigenvalue/variance for leading eigenvalue 0 = 0.15115807186715166\n",
      "eigenvalue/variance for leading eigenvalue 1 = 0.10300032709372527\n",
      "eigenvalue/variance for leading eigenvalue 2 = 0.0937513018331755\n",
      "eigenvalue/variance for leading eigenvalue 3 = 0.08374712329075978\n",
      "eigenvalue/variance for leading eigenvalue 4 = 0.08169285322720388\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "pca = PCA().fit(x_train)\n",
    "print(np.cumsum(pca.explained_variance_ratio_))\n",
    "# From the output array it can be seen that 5 components (0th component counted as 1st) are needed to explain >= 50% of the variance\n",
    "\n",
    "eigenval = pca.explained_variance_\n",
    "print(eigenval)\n",
    "var = np.sum(eigenval)\n",
    "frac = eigenval/var\n",
    "print(frac) # same as explained_variance_ratio as required.\n",
    "\n",
    "# first 5 componets are \n",
    "for i in range(5):\n",
    "    print(f'eigenvalue/variance for leading eigenvalue {i} = {frac[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf7089",
   "metadata": {},
   "source": [
    "## Problem 3 (40 points): AutoEncoders for Anomaly Detection\n",
    "\n",
    "We discussed the use of AutoEncoders for de-noising, let's also look at AutoEncoders in the context of anomaly detection.  The goal will be to use an AutoEncoder to find \"anomalies\" in a data sample without knowing what those anomalies are.  We'll do this by training the network on an un-labeled data sample (i.e. unsupervised learning) and then using the trained network to evaluate a test dataset that has anomalies included.  The anomalies should show up as events with large loss values.\n",
    "\n",
    "For this problem, we'll use the `ttbar` dataset from problem 2 for training, and the `testdata` dataset from problem 2 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15df0cb",
   "metadata": {},
   "source": [
    "**Your job**: Define our AutoEncoder, following the scheme we used in the AutoEncoder notebook in class, with the following changes:\n",
    "\n",
    "* The number of input variables should be the number of variables needed to explain 50% of the variance in the `ttbar` sample (see solutions to Problem 2, let's call this number N_50).\n",
    "* Use the `LeakyReLU` activation function, with args = `(0.2,inplace-True)`\n",
    "* The encoder should have three layers:  (1) the input layer, with N nodes; (2) a hidden layer with 2\\*N nodes; (3) a hidden layer with 10 nodes; (4) the latent-space layer, with N_50 nodes.\n",
    "* Make sure to not define an activation on the last layer!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e8678af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self,n):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = torch.nn.Sequential(\n",
    "                torch.nn.Linear(n, 2*n),\n",
    "                torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "                torch.nn.Linear(2*n, 10),\n",
    "                torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "                torch.nn.Linear(10, 5),\n",
    "                torch.nn.LeakyReLU(0.2, inplace=True),\n",
    "                torch.nn.Linear(5, 5), # latent space layer\n",
    "                )\n",
    "            \n",
    "        self.decoder = nn.Linear(5, n)\n",
    "        self.data_rho=None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        self.data_rho = x.mean(0)\n",
    "        x = self.decoder(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b3e137",
   "metadata": {},
   "source": [
    "Now train the autoencoder on the training data.  Run for 1500 epochs, using the `Adam`optimizer with a learning rate of 0.01.  Afterwards:\n",
    "* Make a plot of the loss function vs epochs for the training data\n",
    "* Make a plot of the loss *values* for all events in the training dataset.  This means passing single events into the loss function, instead of entire tensors!\n",
    "* Now put the net into evaluation mode, and pass the test dataset through the network.  Make the same plot of loss values for all events in the test dataset.  \n",
    "* Make a single plot that includes both histograms: the training set, and the testing set.\n",
    "\n",
    "The difference between the training data and the test data can be subtle unless you overlay them, and (hopefully) see that there are more events in the large-loss tail of the test distribution than the training distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e9659b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9520, 17)\n",
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Linear(in_features=10, out_features=5, bias=True)\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Linear(in_features=5, out_features=5, bias=True)\n",
      "  )\n",
      "  (decoder): Linear(in_features=5, out_features=5, bias=True)\n",
      ")\n",
      "9520\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8084\\3294587007.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1500\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8084\\2940375251.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_rho\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "pca = PCA(5)\n",
    "train = torch.tensor(pca.fit_transform(x_train),dtype=torch.float)\n",
    "\n",
    "\n",
    "print(x_train.shape)\n",
    "\n",
    "net = AutoEncoder(5)\n",
    "print(net)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "print(len(x_train))\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(1500):\n",
    "    start_time = time.time()\n",
    "    pred = net(x_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(pred, x_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.data)\n",
    "    \n",
    "    net.eval() # configure the model for evaluation (testing)\n",
    "    pred_test = net(x_test)\n",
    "    test_loss=loss_fn(pred_test, x_test)\n",
    "    losses_test.append(test_loss.data)\n",
    "    end_time=time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"Completed epoch %2d in %4.1fs, Train loss=%4.1e, Test loss=%4.1e\" % (epoch, elapsed_time, loss.data, test_loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1557ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses vs epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7022a",
   "metadata": {},
   "source": [
    "A helper function to retrieve the individual event-wise losses from the sample (`getLosses`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13341fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "def getLosses(sample):\n",
    "    losses=[]\n",
    "    for i in range(len(sample)):\n",
    "        pred = net(sample[i])\n",
    "        loss = loss_fn(pred, sample[i])\n",
    "        losses.append(loss.data.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87061e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training dataset losses and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd09162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test dataset losses and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f218dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make a single plot overlaying the training and test dataset losses, \n",
    "# with each normalized so they both have unit area."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "import os.path\n",
    "from os.path import exists\n",
    "import os\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import h5py\n",
    "import urllib.request\n",
    "\n",
    "device=\"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    device=\"cuda:0\"\n",
    "print(device)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 (40 points): Revisiting our CNN from lecture\n",
    "\n",
    "Let's revisit the CNN notebook that we worked through in class, and modify the code there to do something slightly different.\n",
    "\n",
    "First, let's import our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 'ising_data.npz'\n",
    "\n",
    "if not exists(output):\n",
    "    import gdown\n",
    "    url = \"https://drive.google.com/u/0/uc?export=download&confirm=HgGH&id=1Ihxt1hb3Kyv0IrjHlsYb9x9QY7l7n2Sl\"\n",
    "    gdown.download(url, output, quiet=True)\n",
    "\n",
    "f = np.load(output, allow_pickle=True)\n",
    "n_train = 20000\n",
    "n_test=len(f[\"C\"])-n_train\n",
    "\n",
    "x_train, x_test = f[\"C\"][:n_train], f[\"C\"][n_train:]\n",
    "y_train, y_test = f[\"T\"][:n_train], f[\"T\"][n_train:]\n",
    "\n",
    "x_train_tensor=torch.tensor(x_train,dtype=torch.float).unsqueeze(1)\n",
    "y_train_tensor=torch.tensor(y_train,dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "x_test_tensor=torch.tensor(x_test,dtype=torch.float).unsqueeze(1)\n",
    "y_test_tensor=torch.tensor(y_test,dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "xy_train = torch.utils.data.TensorDataset(x_train_tensor.float(),y_train_tensor)\n",
    "\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CNN lecture notebook, we used a CNN to predict whether the image had crossed the phase transition boundary or not.  What I'd like to do is a followup that I may have mentioned in lecture: let's use the CNN to predict the temperature that corresponds to the image, instead of a binary \"yes/no\" decision.  Some things to consider:\n",
    "\n",
    "* What should the loss function be?\n",
    "\n",
    "**Now your job**: setup and train a CNN that predicts the temperature associated with each image.  Here are some requirements, which may be different from what was done in the CNN notebook in class:\n",
    "\n",
    "* Have the first convolutional layer use 16 filters\n",
    "* Have the second convolutional layer use 32 filters\n",
    "* Have the fully-connected part at the end include two layers with 32 nodes\n",
    "* Use the `Adam` optimizer with a learning rate of 0.01\n",
    "* Run for 10 epochs\n",
    "\n",
    "\n",
    "As part of this problem, you should produce:\n",
    "* The normal loss plot showing the training and test losses vs the number of epochs\n",
    "* Once the network is trained, pass the test data through the network again, and get the predicted values.  Compare the predicted values with the true values; plot the difference and find the bias and variance of the network.\n",
    "\n",
    "Some hints:\n",
    "* You don't necessarily need to run the code for the full 16+32 convolutional filter network every time, the training takes a little while!  Feel free to debug with smaller numbers of filters.\n",
    "* However, the setup is a bit delicate, so even small changes in the hyperparameters specified above may cause the training to not converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code defining the AutoEncoder class goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the net, define the loss fuction and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate and plot the losses for individual training dataset events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate and plot the losses for individual training dataset events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 (20 points): Principal Components Analysis\n",
    "\n",
    "Let's perform a PCA test on some input data that we'll use for Problem #3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's get some training and test datasets that we can use...  we'll start with one of the files that we used from HW3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sample(sample):\n",
    "    events=[]\n",
    "    for i in sample:\n",
    "        if i[0]>2:\n",
    "            events.append(i)\n",
    "    return events\n",
    "\n",
    "def get_sample_from_URL(samplename,branches):\n",
    "    \n",
    "    if not os.path.isfile(samplename):\n",
    "        urllib.request.urlretrieve(\"http://mhance.scipp.ucsc.edu/%s\" % samplename, samplename)\n",
    "\n",
    "    alldata=None\n",
    "    with h5py.File(samplename,'r') as hdf5file:\n",
    "        data=hdf5file[list(hdf5file.keys())[0]][\"lowleveltree\"]\n",
    "        num_backgr_events=len(data[\"numjet\"])\n",
    "        alldata = data[branches]    \n",
    "\n",
    "    Alldata=[[float(i) for i in j] for j in alldata]\n",
    "    Alldata_clean=clean_sample(Alldata)\n",
    "    return Alldata_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now scale based on the training data:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "branches=(\"numjet\",\"numlepton\",\"numbtagjet\",\n",
    "          \"met\",\"metphi\",\n",
    "          \"jet1pT\", \"jet1eta\", \"jet1phi\",\"jet1b\",\n",
    "          \"jet2pT\", \"jet2eta\", \"jet2phi\",\"jet2b\",\n",
    "          \"jet3pT\", \"jet3eta\", \"jet3phi\",\"jet3b\")\n",
    "\n",
    "traindata=get_sample_from_URL('lowlevelAna_ttbar.hf5',branches)\n",
    "x_train = sc.fit_transform(traindata)\n",
    "train=torch.tensor(x_train,dtype=torch.float)\n",
    "\n",
    "testdata=get_sample_from_URL('lowlevelAna_test.hf5',branches)\n",
    "x_test = sc.transform(testdata)\n",
    "test=torch.tensor(x_test,dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `scikit-learn` PCA tool, analyze the `ttbar` dataset (`x_train`) retrieved above.\n",
    "\n",
    "* Find the number of components needed to explain 50% of the variance.\n",
    "* List the magnitudes of that number of leading eigenvalues, as a fraction of the total variance they capture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 (40 points): AutoEncoders for Anomaly Detection\n",
    "\n",
    "We discussed the use of AutoEncoders for de-noising, let's also look at AutoEncoders in the context of anomaly detection.  The goal will be to use an AutoEncoder to find \"anomalies\" in a data sample without knowing what those anomalies are.  We'll do this by training the network on an un-labeled data sample (i.e. unsupervised learning) and then using the trained network to evaluate a test dataset that has anomalies included.  The anomalies should show up as events with large loss values.\n",
    "\n",
    "For this problem, we'll use the `ttbar` dataset from problem 2 for training, and the `testdata` dataset from problem 2 for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your job**: Define our AutoEncoder, following the scheme we used in the AutoEncoder notebook in class, with the following changes:\n",
    "\n",
    "* The number of input variables should be the number of variables needed to explain 50% of the variance in the `ttbar` sample (see solutions to Problem 2, let's call this number N_50).\n",
    "* Use the `LeakyReLU` activation function, with args = `(0.2,inplace-True)`\n",
    "* The encoder should have three layers:  (1) the input layer, with N nodes; (2) a hidden layer with 2\\*N nodes; (3) a hidden layer with 10 nodes; (4) the latent-space layer, with N_50 nodes.\n",
    "* Make sure to not define an activation on the last layer!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the autoencoder on the training data.  Run for 1500 epochs, using the `Adam`optimizer with a learning rate of 0.01.  Afterwards:\n",
    "* Make a plot of the loss function vs epochs for the training data\n",
    "* Make a plot of the loss *values* for all events in the training dataset.  This means passing single events into the loss function, instead of entire tensors!\n",
    "* Now put the net into evaluation mode, and pass the test dataset through the network.  Make the same plot of loss values for all events in the test dataset.  \n",
    "* Make a single plot that includes both histograms: the training set, and the testing set.\n",
    "\n",
    "The difference between the training data and the test data can be subtle unless you overlay them, and (hopefully) see that there are more events in the large-loss tail of the test distribution than the training distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses vs epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to retrieve the individual event-wise losses from the sample (`getLosses`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "def getLosses(sample):\n",
    "    losses=[]\n",
    "    for i in range(len(sample)):\n",
    "        pred = net(sample[i])\n",
    "        loss = loss_fn(pred, sample[i])\n",
    "        losses.append(loss.data.item())\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the training dataset losses and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the test dataset losses and plot them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a single plot overlaying the training and test dataset losses, \n",
    "# with each normalized so they both have unit area."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
